1.확률적 경사하강법에대해 설명하시오
전체 데이터를 사용하며 하나의 샘플 혹은 소규모의 배치로 학습 반복마다 손실함수의 gradient를 계산하여 가중치를 업데이트 하는 방법

2. Dropout에 대해 설명하시오
학습중 일부 뉴런을 랜덤하게 비활성화하여 과적합을 방지하는 정규화 기법이다. 모델이 특정 뉴런에 과도하게 치중되어 학습이 편향되지 않게 하고 앙상블 효과도 유사하게 낼 수 있다.

3.RNN과 CNN에 대해 차이점을 설명하시오


4. LSTM에 CELL에 대해 설명하고 각 게이트에 대해 간단히 설명하시오
RNN의 장기 의존성 문제를 해결한 학습방법 모델이다.
cell은 정보흐름을 조절하는 3개의 게이트를 사용하는데
INPUT GATE: 현재 입력을 얼마나 반영할지 결정
OUTPUT GATE: 현재 상태에서 무엇을 출력할지 결정
FORGET GATE: 과거 정보를 얼마나 유지할지 결정

5.accuracy보다 f1-score를 사용하는 이유를 설명하시오.
Accuracy는 전체 중 맞춘 비율을 보유주지만, 레이블 불균형이 심한 겨웅에는 잘못된 판단일 수 있다.
F1-score는 precision과 recall의 조화 평균으로 (여기까진 똑같이 씀)

불균형 데이터에서도 모델 성능을 더 공정하게 평가 가능함.


6. 레이블 불균형이 모델에 어떤 나쁜 영향을 미치는지 자세히 기술하고 해결법을 기술하시오.
모델이 다수 클래스에 편향 되어서 소수 클래스가 무시됨. (여기도 비슷하게 씀)

해결법은 resampling, 클래스 가중치 조정
Resampling: 소수 데이터를 증강 하거나, 다수 클래스 데이터를 일부 제거함
클래스 가중치 조정: 손실 함수에 클래스별 가중치를 부옇마.


7.풀링의 두가지 방법을 쓰고 max, avg pooling에 대해 각각 설명하시오.
(풀링 두개는 맞게 적었음)
Max Pooling: 지정영역 내에 가장 큰값만 선택함, 특징의 가장 뚜렷한 부분을 갖오하여 계산향 감소에 유리함.
Avg Pooling: 지정된 영역 내의 모든 값의 평균을 사용함, 전체적인 정보 유지에 유리하나, 특징 강조는 덜함.

